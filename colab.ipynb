{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri524ORh18Wx"
      },
      "source": [
        "Copyright 2024 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGIIQJ5IN34L"
      },
      "source": [
        "# InkSight: OÔ¨Ñine-to-Online Handwriting Conversion by Teaching Vision-Language Models to Read and Write\n",
        "<p align=\"center\">\n",
        "  <a href=\"https://research.google/blog/a-return-to-hand-written-notes-by-learning-to-read-write/\">\n",
        "    <img src=\"https://img.shields.io/badge/Google_Research_Blog-333333?&logo=google&logoColor=white\" alt=\"Google Research Blog\">\n",
        "  </a>\n",
        "  <a href=\"https://arxiv.org/abs/2402.05804\">\n",
        "    <img src=\"https://img.shields.io/badge/Read_the_Paper-4CAF50?&logo=arxiv&logoColor=white\" alt=\"Read the Paper\">\n",
        "  </a>\n",
        "  <a href=\"https://huggingface.co/spaces/Derendering/Model-Output-Playground\">\n",
        "    <img src=\"https://img.shields.io/badge/Output_Playground-007acc?&logo=huggingface&logoColor=white\" alt=\"Try Demo on Hugging Face\">\n",
        "  </a>\n",
        "    <a href=\"https://charlieleee.github.io/publication/inksight/\">\n",
        "    <img src=\"https://img.shields.io/badge/üîó_Project_Page-FFA500?&logo=link&logoColor=white\" alt=\"Project Page\">\n",
        "  </a>\n",
        "  <a href=\"https://huggingface.co/datasets/Derendering/InkSight-Derenderings\">\n",
        "    <img src=\"https://img.shields.io/badge/Dataset-InkSight-40AF40?&logo=huggingface&logoColor=white\" alt=\"Hugging Face Dataset\">\n",
        "  </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRqyxme1ZCyZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "958b69b4-35bb-412a-e076-6faeac34ef71"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## üì¶ Installing required packages...\nThis may take a minute. Please wait..."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Dependencies\n",
        "from IPython.display import Markdown\n",
        "import time\n",
        "import os\n",
        "\n",
        "display(Markdown(\"## üì¶ Installing required packages...\\nThis may take a minute. Please wait...\"))\n",
        "\n",
        "!sudo apt -qq install tesseract-ocr\n",
        "!uv pip install -q --system \"tensorflow[and-cuda]==2.17.0\" \"tensorflow-text==2.17.0\" pytesseract \"tf-keras==2.17.0\" \"python-doctr[tf,viz]==0.10.0\"\n",
        "display(Markdown(\"‚úÖ **Installation complete!**\"))\n",
        "time.sleep(1)\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "---\n",
        "### üîÑ Restarting Runtime\n",
        "To finalize the installation, we need to restart the Colab runtime.\n",
        "\n",
        "> **Why?** TensorFlow and system-level packages need a restart to properly initialize with new dependencies.\n",
        "\n",
        "‚è≥ Restarting in 3 seconds...\n",
        "\n",
        "‚úÖ Restartng done! **Please continue to the following cells**\n",
        "\"\"\"))\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "# Kill the current process to force a runtime restart\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xt-15DJo4KK"
      },
      "outputs": [],
      "source": [
        "# @title docTR Preparation\n",
        "\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor\n",
        "predictor = ocr_predictor(pretrained=True)\n",
        "print(\"doctr predictor loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bObP17TGnT8x"
      },
      "outputs": [],
      "source": [
        "# @title ####Utils\n",
        "from IPython.display import SVG, display\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "import gdown\n",
        "import os\n",
        "from PIL import Image, ImageEnhance, ImageDraw\n",
        "import matplotlib.animation as animation\n",
        "import copy\n",
        "import colorsys\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.collections import LineCollection\n",
        "from matplotlib.patheffects import withStroke\n",
        "import random\n",
        "import IPython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "from matplotlib.figure import Figure\n",
        "from io import BytesIO\n",
        "from matplotlib.animation import FuncAnimation, FFMpegWriter, PillowWriter\n",
        "import requests\n",
        "import zipfile\n",
        "import base64\n",
        "\n",
        "\n",
        "def get_svg_content(svg_path):\n",
        "    with open(svg_path, \"r\") as file:\n",
        "        return file.read()\n",
        "\n",
        "\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url)\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "\n",
        "def unzip_file(filename, extract_to=\".\"):\n",
        "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "\n",
        "def get_base64_encoded_gif(gif_path):\n",
        "    with open(gif_path, \"rb\") as gif_file:\n",
        "        return base64.b64encode(gif_file.read()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def load_and_pad_img_dir(file_dir):\n",
        "    image_path = os.path.join(file_dir)\n",
        "    image = Image.open(image_path)\n",
        "    width, height = image.size\n",
        "    ratio = min(224 / width, 224 / height)\n",
        "    image = image.resize((int(width * ratio), int(height * ratio)))\n",
        "    width, height = image.size\n",
        "    if height < 224:\n",
        "        # If width is shorter than height pad top and bottom.\n",
        "        top_padding = (224 - height) // 2\n",
        "        bottom_padding = 224 - height - top_padding\n",
        "        padded_image = Image.new(\"RGB\", (width, 224), (255, 255, 255))\n",
        "        padded_image.paste(image, (0, top_padding))\n",
        "    else:\n",
        "        # Otherwise pad left and right.\n",
        "        left_padding = (224 - width) // 2\n",
        "        right_padding = 224 - width - left_padding\n",
        "        padded_image = Image.new(\"RGB\", (224, height), (255, 255, 255))\n",
        "        padded_image.paste(image, (left_padding, 0))\n",
        "    return padded_image\n",
        "\n",
        "\n",
        "def plot_ink(ink, ax, lw=1.8, input_image=None, with_path=True, path_color=\"white\"):\n",
        "    if input_image is not None:\n",
        "        img = copy.deepcopy(input_image)\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(0.45)\n",
        "        ax.imshow(img)\n",
        "\n",
        "    base_colors = plt.cm.get_cmap(\"rainbow\", len(ink.strokes))\n",
        "\n",
        "    for i, stroke in enumerate(ink.strokes):\n",
        "        x, y = np.array(stroke.x), np.array(stroke.y)\n",
        "\n",
        "        base_color = base_colors(len(ink.strokes) - 1 - i)\n",
        "        hsv_color = colorsys.rgb_to_hsv(*base_color[:3])\n",
        "\n",
        "        darker_color = colorsys.hsv_to_rgb(\n",
        "            hsv_color[0], hsv_color[1], max(0, hsv_color[2] * 0.65)\n",
        "        )\n",
        "        colors = [\n",
        "            mcolors.to_rgba(darker_color, alpha=1 - (0.5 * j / len(x)))\n",
        "            for j in range(len(x))\n",
        "        ]\n",
        "\n",
        "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "        lc = LineCollection(segments, colors=colors, linewidth=lw)\n",
        "        if with_path:\n",
        "            lc.set_path_effects(\n",
        "                [withStroke(linewidth=lw * 1.25, foreground=path_color)]\n",
        "            )\n",
        "        ax.add_collection(lc)\n",
        "\n",
        "    ax.set_xlim(0, 224)\n",
        "    ax.set_ylim(0, 224)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "\n",
        "def plot_ink_to_video(\n",
        "    ink, output_name, lw=1.8, input_image=None, path_color=\"white\", fps=30\n",
        "):\n",
        "    fig, ax = plt.subplots(figsize=(4, 4), dpi=150)\n",
        "\n",
        "    if input_image is not None:\n",
        "        img = copy.deepcopy(input_image)\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(0.45)\n",
        "        ax.imshow(img)\n",
        "\n",
        "    ax.set_xlim(0, 224)\n",
        "    ax.set_ylim(0, 224)\n",
        "    ax.invert_yaxis()\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    base_colors = plt.cm.get_cmap(\"rainbow\", len(ink.strokes))\n",
        "    all_points = sum([len(stroke.x) for stroke in ink.strokes], 0)\n",
        "\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        if input_image is not None:\n",
        "            ax.imshow(img)\n",
        "        ax.set_xlim(0, 224)\n",
        "        ax.set_ylim(0, 224)\n",
        "        ax.invert_yaxis()\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        points_drawn = 0\n",
        "        for stroke_index, stroke in enumerate(ink.strokes):\n",
        "            x, y = np.array(stroke.x), np.array(stroke.y)\n",
        "            points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "            base_color = base_colors(len(ink.strokes) - 1 - stroke_index)\n",
        "            hsv_color = colorsys.rgb_to_hsv(*base_color[:3])\n",
        "            darker_color = colorsys.hsv_to_rgb(\n",
        "                hsv_color[0], hsv_color[1], max(0, hsv_color[2] * 0.65)\n",
        "            )\n",
        "            visible_segments = (\n",
        "                segments[: frame - points_drawn]\n",
        "                if frame - points_drawn < len(segments)\n",
        "                else segments\n",
        "            )\n",
        "            colors = [\n",
        "                mcolors.to_rgba(\n",
        "                    darker_color, alpha=1 - (0.5 * j / len(visible_segments))\n",
        "                )\n",
        "                for j in range(len(visible_segments))\n",
        "            ]\n",
        "\n",
        "            if len(visible_segments) > 0:\n",
        "                lc = LineCollection(visible_segments, colors=colors, linewidth=lw)\n",
        "                lc.set_path_effects(\n",
        "                    [withStroke(linewidth=lw * 1.25, foreground=path_color)]\n",
        "                )\n",
        "                ax.add_collection(lc)\n",
        "\n",
        "            points_drawn += len(segments)\n",
        "            if points_drawn >= frame:\n",
        "                break\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=all_points + 1, blit=False)\n",
        "    Writer = FFMpegWriter(fps=fps)\n",
        "    plt.tight_layout()\n",
        "    ani.save(output_name, writer=Writer)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "class Stroke:\n",
        "    def __init__(self, list_of_coordinates=None) -> None:\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        if list_of_coordinates:\n",
        "            for point in list_of_coordinates:\n",
        "                self.x.append(point[0])\n",
        "                self.y.append(point[1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.x[index], self.y[index])\n",
        "\n",
        "\n",
        "class Ink:\n",
        "    def __init__(self, list_of_strokes=None) -> None:\n",
        "        self.strokes = []\n",
        "        if list_of_strokes:\n",
        "            self.strokes = list_of_strokes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.strokes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.strokes[index]\n",
        "\n",
        "\n",
        "def inkml_to_ink(inkml_file):\n",
        "    \"\"\"Convert inkml file to Ink\"\"\"\n",
        "    tree = ET.parse(inkml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    inkml_namespace = {\"inkml\": \"http://www.w3.org/2003/InkML\"}\n",
        "\n",
        "    strokes = []\n",
        "\n",
        "    for trace in root.findall(\"inkml:trace\", inkml_namespace):\n",
        "        points = trace.text.strip().split()\n",
        "        stroke_points = []\n",
        "\n",
        "        for point in points:\n",
        "            x, y = point.split(\",\")\n",
        "            stroke_points.append((float(x), float(y)))\n",
        "        strokes.append(Stroke(stroke_points))\n",
        "    return Ink(strokes)\n",
        "\n",
        "\n",
        "def parse_inkml_annotations(inkml_file):\n",
        "    tree = ET.parse(inkml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    annotations = root.findall(\".//{http://www.w3.org/2003/InkML}annotation\")\n",
        "\n",
        "    annotation_dict = {}\n",
        "\n",
        "    for annotation in annotations:\n",
        "        annotation_type = annotation.get(\"type\")\n",
        "        annotation_text = annotation.text\n",
        "\n",
        "        annotation_dict[annotation_type] = annotation_text\n",
        "\n",
        "    return annotation_dict\n",
        "\n",
        "\n",
        "def pregenerate_videos(video_cache_dir):\n",
        "    datasets = [\"IAM\", \"IMGUR5K\", \"HierText\"]\n",
        "    models = [\"Small-i\", \"Large-i\", \"Small-p\"]\n",
        "    query_modes = [\"d+t\", \"r+d\", \"vanilla\"]\n",
        "    for Dataset in datasets:\n",
        "        for Model in models:\n",
        "            inkml_path_base = f\"./derendering_supp/{Model.lower()}_{Dataset}_inkml\"\n",
        "            for mode in query_modes:\n",
        "                path = f\"./derendering_supp/{Dataset}/images_sample\"\n",
        "                if not os.path.exists(path):\n",
        "                    continue\n",
        "                samples = os.listdir(path)\n",
        "                for name in tqdm(\n",
        "                    samples, desc=f\"Generating {Model}-{Dataset}-{mode} videos\"\n",
        "                ):\n",
        "                    example_id = name.strip(\".png\")\n",
        "                    inkml_file = os.path.join(\n",
        "                        inkml_path_base, mode, f\"{example_id}.inkml\"\n",
        "                    )\n",
        "                    if not os.path.exists(inkml_file):\n",
        "                        continue\n",
        "                    video_filename = f\"{Model}_{Dataset}_{mode}_{example_id}.mp4\"\n",
        "                    video_filepath = video_cache_dir / video_filename\n",
        "                    if not video_filepath.exists():\n",
        "                        img_path = os.path.join(path, name)\n",
        "                        img = load_and_pad_img_dir(img_path)\n",
        "                        ink = inkml_to_ink(inkml_file)\n",
        "                        plot_ink_to_video(ink, str(video_filepath), input_image=img)\n",
        "\n",
        "\n",
        "\n",
        "def show_system():\n",
        "    display(SVG('derendering_supp/derender_diagram.svg'))\n",
        "\n",
        "\n",
        "def load_and_pad_img_dir(file_dir):\n",
        "    image_path = os.path.join(file_dir)\n",
        "    image = Image.open(image_path)\n",
        "    width, height = image.size\n",
        "    ratio = min(224 / width, 224 / height)\n",
        "    image = image.resize((int(width * ratio), int(height * ratio)))\n",
        "    width, height = image.size\n",
        "    if height < 224:\n",
        "        # If width is shorter than height pad top and bottom.\n",
        "        top_padding = (224 - height) // 2\n",
        "        bottom_padding = 224 - height - top_padding\n",
        "        padded_image = Image.new('RGB', (width, 224), (255, 255, 255))\n",
        "        padded_image.paste(image, (0, top_padding))\n",
        "    else:\n",
        "        # Otherwise pad left and right.\n",
        "        left_padding = (224 - width) // 2\n",
        "        right_padding = 224 - width - left_padding\n",
        "        padded_image = Image.new('RGB', (224, height), (255, 255, 255))\n",
        "        padded_image.paste(image, (left_padding, 0))\n",
        "    return padded_image\n",
        "\n",
        "def plot_ink(ink, ax, lw=1.8, input_image=None, with_path=True, path_color='white'):\n",
        "  if input_image is not None:\n",
        "    img = copy.deepcopy(input_image)\n",
        "    enhancer = ImageEnhance.Brightness(img)\n",
        "    img = enhancer.enhance(0.45)\n",
        "    ax.imshow(img)\n",
        "\n",
        "  base_colors = plt.cm.get_cmap('rainbow', len(ink.strokes))\n",
        "\n",
        "  for i, stroke in enumerate(ink.strokes):\n",
        "    x, y = np.array(stroke.x), np.array(stroke.y)\n",
        "\n",
        "    base_color = base_colors(len(ink.strokes) - 1 - i)\n",
        "    hsv_color = colorsys.rgb_to_hsv(*base_color[:3])\n",
        "\n",
        "    darker_color = colorsys.hsv_to_rgb(hsv_color[0], hsv_color[1], max(0, hsv_color[2] * 0.65))\n",
        "    colors = [mcolors.to_rgba(darker_color, alpha=1 - (0.5 * j / len(x))) for j in range(len(x))]\n",
        "\n",
        "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "    lc = LineCollection(segments, colors=colors, linewidth=lw)\n",
        "    if with_path:\n",
        "      lc.set_path_effects([withStroke(linewidth=lw*1.25, foreground=path_color)])\n",
        "    ax.add_collection(lc)\n",
        "\n",
        "  ax.set_xlim(0, 224)\n",
        "  ax.set_ylim(0, 224)\n",
        "  ax.invert_yaxis()\n",
        "\n",
        "def plot_ink_to_gif(ink, output_filename, lw=1.8, input_image=None, path_color='white', fps=30):\n",
        "    fig, ax = plt.subplots(figsize=(4, 4), dpi=150)\n",
        "\n",
        "    if input_image is not None:\n",
        "        img = copy.deepcopy(input_image)\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(0.45)\n",
        "        ax.imshow(img)\n",
        "\n",
        "    base_colors = plt.cm.get_cmap('rainbow', len(ink.strokes))\n",
        "\n",
        "    def get_segments(stroke):\n",
        "        x, y = np.array(stroke.x), np.array(stroke.y)\n",
        "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "        return np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "    all_segments = [get_segments(stroke) for stroke in ink.strokes]\n",
        "    max_frames = sum(len(segments) for segments in all_segments)\n",
        "\n",
        "    def update(frame):\n",
        "        current_frame = 0\n",
        "        for i, segments in enumerate(all_segments):\n",
        "            if current_frame + len(segments) > frame:\n",
        "                segment_index = frame - current_frame\n",
        "                base_color = base_colors(len(ink.strokes) - 1 - i)\n",
        "                hsv_color = colorsys.rgb_to_hsv(*base_color[:3])\n",
        "                darker_color = colorsys.hsv_to_rgb(\n",
        "                    hsv_color[0], hsv_color[1], max(0, hsv_color[2] * 0.65))\n",
        "                colors = [mcolors.to_rgba(\n",
        "                    darker_color, alpha=1 - (0.5 * j / len(segments))) for j in range(len(segments))]\n",
        "\n",
        "                lc = LineCollection(\n",
        "                    segments[:segment_index+1], colors=colors[:segment_index+1], linewidth=lw)\n",
        "                if path_color:\n",
        "                    lc.set_path_effects(\n",
        "                        [withStroke(linewidth=lw*1.25, foreground=path_color)])\n",
        "\n",
        "                ax.add_collection(lc)\n",
        "                break\n",
        "\n",
        "            current_frame += len(segments)\n",
        "\n",
        "        return ax.collections\n",
        "\n",
        "    ax.set_xlim(0, 224)\n",
        "    ax.set_ylim(0, 224)\n",
        "    ax.invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    ax.axis('off')\n",
        "    ani = animation.FuncAnimation(fig, update, frames=max_frames, blit=True)\n",
        "    ani.save(output_filename, writer='imagemagick', fps=fps)\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "class Stroke:\n",
        "    def __init__(self, list_of_coordinates=None) -> None:\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        if list_of_coordinates:\n",
        "            for point in list_of_coordinates:\n",
        "                self.x.append(point[0])\n",
        "                self.y.append(point[1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.x[index], self.y[index])\n",
        "\n",
        "\n",
        "class Ink:\n",
        "    def __init__(self, list_of_strokes=None) -> None:\n",
        "        self.strokes = []\n",
        "        if list_of_strokes:\n",
        "            self.strokes = list_of_strokes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.strokes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.strokes[index]\n",
        "\n",
        "def inkml_to_ink(inkml_file):\n",
        "    \"\"\" Convert inkml file to Ink\"\"\"\n",
        "    tree = ET.parse(inkml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    inkml_namespace = {'inkml': 'http://www.w3.org/2003/InkML'}\n",
        "\n",
        "    strokes = []\n",
        "\n",
        "    for trace in root.findall('inkml:trace', inkml_namespace):\n",
        "        points = trace.text.strip().split()\n",
        "        stroke_points = []\n",
        "\n",
        "        for point in points:\n",
        "            x, y = point.split(',')\n",
        "            stroke_points.append((float(x), float(y)))\n",
        "        strokes.append(Stroke(stroke_points))\n",
        "    return Ink(strokes)\n",
        "\n",
        "def parse_inkml_annotations(inkml_file):\n",
        "  tree = ET.parse(inkml_file)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  annotations = root.findall('.//{http://www.w3.org/2003/InkML}annotation')\n",
        "\n",
        "  annotation_dict = {}\n",
        "\n",
        "  for annotation in annotations:\n",
        "    annotation_type = annotation.get('type')\n",
        "    annotation_text = annotation.text\n",
        "\n",
        "    annotation_dict[annotation_type] = annotation_text\n",
        "\n",
        "  return annotation_dict\n",
        "\n",
        "def plot_ink_to_video(\n",
        "    ink, output_name, lw=1.8, input_image=None, path_color=\"white\", fps=30\n",
        "):\n",
        "    fig, ax = plt.subplots(figsize=(4, 4), dpi=150)\n",
        "\n",
        "    if input_image is not None:\n",
        "        img = copy.deepcopy(input_image)\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(0.45)\n",
        "        ax.imshow(img)\n",
        "\n",
        "    ax.set_xlim(0, 224)\n",
        "    ax.set_ylim(0, 224)\n",
        "    ax.invert_yaxis()\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    base_colors = plt.cm.get_cmap(\"rainbow\", len(ink.strokes))\n",
        "    all_points = sum([len(stroke.x) for stroke in ink.strokes], 0)\n",
        "\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        if input_image is not None:\n",
        "            ax.imshow(img)\n",
        "        ax.set_xlim(0, 224)\n",
        "        ax.set_ylim(0, 224)\n",
        "        ax.invert_yaxis()\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        points_drawn = 0\n",
        "        for stroke_index, stroke in enumerate(ink.strokes):\n",
        "            x, y = np.array(stroke.x), np.array(stroke.y)\n",
        "            points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "            base_color = base_colors(len(ink.strokes) - 1 - stroke_index)\n",
        "            hsv_color = colorsys.rgb_to_hsv(*base_color[:3])\n",
        "            darker_color = colorsys.hsv_to_rgb(\n",
        "                hsv_color[0], hsv_color[1], max(0, hsv_color[2] * 0.65)\n",
        "            )\n",
        "            visible_segments = (\n",
        "                segments[: frame - points_drawn]\n",
        "                if frame - points_drawn < len(segments)\n",
        "                else segments\n",
        "            )\n",
        "            colors = [\n",
        "                mcolors.to_rgba(\n",
        "                    darker_color, alpha=1 - (0.5 * j / len(visible_segments))\n",
        "                )\n",
        "                for j in range(len(visible_segments))\n",
        "            ]\n",
        "\n",
        "            if len(visible_segments) > 0:\n",
        "                lc = LineCollection(visible_segments, colors=colors, linewidth=lw)\n",
        "                lc.set_path_effects(\n",
        "                    [withStroke(linewidth=lw * 1.25, foreground=path_color)]\n",
        "                )\n",
        "                ax.add_collection(lc)\n",
        "\n",
        "            points_drawn += len(segments)\n",
        "            if points_drawn >= frame:\n",
        "                break\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=all_points + 1, blit=False)\n",
        "    Writer = animation.FFMpegWriter(fps=fps)\n",
        "    ani.save(output_name, writer=Writer)\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gmulR0nDf2aK"
      },
      "outputs": [],
      "source": [
        "# @title #### Preparation\n",
        "!rm -rf derendering_supp/ derendering_supp.zip __MACOSX\n",
        "!wget https://storage.googleapis.com/derendering_model/derendering_supp.zip\n",
        "!unzip -q derendering_supp.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sT0WaDDvozKD"
      },
      "outputs": [],
      "source": [
        "# @title # InkSight Overview\n",
        "show_system()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xhizAjs9UTI"
      },
      "source": [
        "# Visualize the generated Digital Ink (Pre-saved Model Outputs)\n",
        "\n",
        "We provide the pre-saved model outputs from three variants (**Small-i**, **Small-p**, **Large-i**) of our models as described in the paper with the three inference modes corresponding to each column below (Derender with Text, Recognize and Derender, Vanilla Derender)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhIaiCr3Uaq2"
      },
      "source": [
        "# Inference with the Public Small-p Model\n",
        "\n",
        "This section demonstrates inference examples using the Small-p model from our paper, both with the fullpage InkSight pipeline (with open source Tesseract OCR or docTR) and at the word-level.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6Ej8cMSz0X8Q"
      },
      "outputs": [],
      "source": [
        "# @title Notice\n",
        "from IPython.display import HTML\n",
        "display(HTML('<p style=\"font-size:20px; font-weight:bold; color:red; background-color:lightgray; padding:10px; width:50%\">Notice on Model Release</p>'))\n",
        "display(HTML('<p style=\"font-size:16px; width:50%\">Model download will be available once the release process is complete. For optimal performance, please use a T4 GPU runtime in colab. </p>'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AbcLNpxhUxDl"
      },
      "outputs": [],
      "source": [
        "# @title Utils\n",
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "import gdown\n",
        "import os\n",
        "import matplotlib.animation as animation\n",
        "import copy\n",
        "from PIL import ImageEnhance, Image, ImageDraw\n",
        "import colorsys\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.collections import LineCollection\n",
        "from matplotlib.patheffects import withStroke\n",
        "import random\n",
        "import warnings\n",
        "import re\n",
        "import time\n",
        "import io\n",
        "import pytesseract\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor\n",
        "\n",
        "\n",
        "def get_box(data_idx, data):\n",
        "    min_x = left = data['left'][data_idx]\n",
        "    min_y = top = data['top'][data_idx]\n",
        "    width = data['width'][data_idx]\n",
        "    angle = 0\n",
        "    height = data['height'][data_idx]\n",
        "    angle = angle / 180.0 * np.pi\n",
        "    s_x = left + np.cos(angle) * width\n",
        "    s_y = top - np.sin(angle) * width\n",
        "    f_x = (\n",
        "        left + np.sin(angle) * height\n",
        "    )\n",
        "    f_y = top + np.cos(angle) * height\n",
        "    max_x = (\n",
        "        left\n",
        "        + np.cos(angle) * width\n",
        "        + np.sin(angle) * height\n",
        "    )\n",
        "    max_y = (\n",
        "        top\n",
        "        - np.sin(angle) * width\n",
        "        + np.cos(angle) * height\n",
        "    )\n",
        "    return min_x, min_y, s_x, s_y, f_x, f_y, max_x, max_y\n",
        "\n",
        "def rotate_crop_scale_and_pad(original, data_idx, data, pad_black=True):\n",
        "    angle = 0\n",
        "    height = data['height'][data_idx]\n",
        "    width = data['width'][data_idx]\n",
        "    min_x, min_y, s_x, s_y, f_x, f_y, _, _ = get_box(data_idx, data)\n",
        "    max_x = min_x + width\n",
        "    max_y = min_y + height\n",
        "\n",
        "    output = original.rotate(angle, center=(min_x, min_y))\n",
        "    crop = output.crop((min_x, min_y, max_x, max_y))\n",
        "\n",
        "    ratio = min(224 / crop.width, 224 / crop.height)\n",
        "    new_crop = crop.resize((int(crop.width * ratio), int(crop.height * ratio)))\n",
        "    new_crop_np = np.array(new_crop)\n",
        "\n",
        "    pixel_1 = new_crop_np[1, 1]\n",
        "    pixel_2 = new_crop_np[1, new_crop_np.shape[-1] - 1]\n",
        "    pixel_3 = new_crop_np[new_crop_np.shape[0] - 1, 1]\n",
        "    pixel_4 = new_crop_np[new_crop_np.shape[0] - 1, new_crop_np.shape[-1] - 1]\n",
        "    avg = np.rint(np.mean([pixel_1, pixel_2, pixel_3, pixel_4], axis=0)).astype(\n",
        "        np.uint8\n",
        "    )\n",
        "\n",
        "    color = tuple(avg) if not pad_black else (0, 0, 0)\n",
        "    new_image = Image.new(new_crop.mode, (224, 224), color)\n",
        "    dx = (224 - new_crop.width) // 2\n",
        "    dy = (224 - new_crop.height) // 2\n",
        "    new_image.paste(new_crop, (dx, dy))\n",
        "    return new_image, ratio, dx, dy, min_x, min_y, angle, crop\n",
        "\n",
        "\n",
        "def extract_fullpage(img_source, option=\"tesseract\"):\n",
        "    ret_imgs = []\n",
        "    img_info = []\n",
        "    img_bbox = []\n",
        "    if isinstance(img_source, (bytes, bytearray)):\n",
        "        input_image = Image.open(io.BytesIO(img_source))\n",
        "    elif isinstance(img_source, str):\n",
        "        input_image = Image.open(img_source)\n",
        "    elif isinstance(img_source, Image.Image):\n",
        "        input_image = img_source\n",
        "    else:\n",
        "        raise TypeError(\"img_source must be bytes, str, or PIL.Image\")\n",
        "    if option == \"tesseract\":\n",
        "        data = pytesseract.image_to_data(input_image, output_type=pytesseract.Output.DICT)\n",
        "        for i in tqdm(range(len(data['text']))):\n",
        "            if data['text'][i].strip() != '':  # Filters out empty text results\n",
        "                new_image, ratio, dx, dy, min_x, min_y, angle, _ = (\n",
        "                    rotate_crop_scale_and_pad(input_image, i, data, pad_black=True)\n",
        "                )\n",
        "                x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
        "                ret_imgs.append(new_image)\n",
        "                img_info.append((ratio, dx, dy, min_x, min_y, angle))\n",
        "                img_bbox.append((x, y, w, h))\n",
        "    elif option == \"doctr\":\n",
        "        doc = DocumentFile.from_images(img_source)\n",
        "        predictor = ocr_predictor(pretrained=True)\n",
        "        print(\"doctr predictor loaded.\")\n",
        "        result = predictor(doc)\n",
        "\n",
        "        for page in result.pages:\n",
        "            for block in page.blocks:\n",
        "                for line in block.lines:\n",
        "                    for word in line.words:\n",
        "                        if word.value.strip() != '':\n",
        "                            coords = word.geometry\n",
        "                            x0, y0 = int(coords[0][0] * input_image.width), int(coords[0][1] * input_image.height)\n",
        "                            x1, y1 = int(coords[1][0] * input_image.width), int(coords[1][1] * input_image.height)\n",
        "                            w, h = x1 - x0, y1 - y0\n",
        "\n",
        "                            w_expand = w * 0.1\n",
        "                            h_expand = h * 0.1\n",
        "\n",
        "                            x0 = max(0, x0 - w_expand)\n",
        "                            y0 = max(0, y0 - h_expand)\n",
        "                            x1 = min(input_image.width, x1 + w_expand)\n",
        "                            y1 = min(input_image.height, y1 + h_expand)\n",
        "\n",
        "                            w = x1 - x0\n",
        "                            h = y1 - y0\n",
        "\n",
        "                            x0, y0, w, h = map(int, [x0, y0, w, h])\n",
        "\n",
        "                            # Create a mock data dictionary similar to tesseract's output\n",
        "                            mock_data = {\n",
        "                                'left': [x0],\n",
        "                                'top': [y0],\n",
        "                                'width': [w],\n",
        "                                'height': [h],\n",
        "                                'conf': [1.0],  # doctr doesn't provide confidence scores in the same way\n",
        "                                'text': [word.value]\n",
        "                            }\n",
        "\n",
        "                            # Use the same processing function as tesseract\n",
        "                            new_image, ratio, dx, dy, min_x, min_y, angle, crop = (\n",
        "                                rotate_crop_scale_and_pad(input_image, 0, mock_data, pad_black=True)\n",
        "                            )\n",
        "\n",
        "                            ret_imgs.append(new_image)\n",
        "                            img_info.append((ratio, dx, dy, min_x, min_y, angle))\n",
        "                            img_bbox.append((x0, y0, w, h))\n",
        "\n",
        "    print('\\nFinal length: ', len(ret_imgs))\n",
        "\n",
        "    print('\\nFinal length: ', len(ret_imgs))\n",
        "\n",
        "    # Draw the bboxes\n",
        "    image = deepcopy(input_image)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for bx in img_bbox:\n",
        "        x, y, w, h = bx\n",
        "        draw.rectangle([x, y, x + w, y + h], outline='red', width=2)\n",
        "\n",
        "    return ret_imgs, img_info, image\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def text_to_tokens(text) -> list[int]:\n",
        "    pattern = r\"<ink_token_(\\d+)>\"\n",
        "    matches = re.findall(pattern, text)\n",
        "    return [int(tok) for tok in matches]\n",
        "\n",
        "def detokenize(tokens: list[int]) -> list[list[tuple[float, float]]]:\n",
        "    coordinate_length = 224\n",
        "    num_token_per_dimension = coordinate_length + 1\n",
        "    vocabulary_size = num_token_per_dimension * 2 + 1\n",
        "    start_token = num_token_per_dimension * 2\n",
        "\n",
        "    if any([t < 0 or t >= vocabulary_size for t in tokens]):\n",
        "        raise ValueError(\n",
        "            f\"Ink token indices should be between 0 and {vocabulary_size}\"\n",
        "        )\n",
        "    idx = 0\n",
        "    res = []\n",
        "    current_stroke_tokens = []\n",
        "\n",
        "    while idx < len(tokens):\n",
        "        token = tokens[idx]\n",
        "        if token == start_token:\n",
        "            if current_stroke_tokens:\n",
        "                res.append(current_stroke_tokens)\n",
        "            current_stroke_tokens = []\n",
        "            idx += 1\n",
        "        else:\n",
        "            if idx + 1 < len(tokens) and (tokens[idx + 1] != start_token):\n",
        "                # Read in x and y coordinates.\n",
        "                x = tokens[idx]\n",
        "                y = tokens[idx + 1] - num_token_per_dimension\n",
        "                # If the coordinates are valid, add them to detokenization ink.\n",
        "                if (0 <= x <= coordinate_length) and (0 <= y <= coordinate_length):\n",
        "                    current_stroke_tokens.append([x, y])\n",
        "                idx += 2\n",
        "            # If y doesn't exist or y is start_token, then skip this x.\n",
        "            else:\n",
        "                idx += 1\n",
        "    if current_stroke_tokens:\n",
        "        res.append(current_stroke_tokens)\n",
        "\n",
        "    strokes = []\n",
        "    for stroke in res:\n",
        "        stroke_points = []\n",
        "        for point in stroke:\n",
        "            x, y = point\n",
        "            stroke_points.append((x, y))\n",
        "        strokes.append(Stroke(stroke_points))\n",
        "    return Ink(strokes)\n",
        "\n",
        "def load_and_pad_img(image):\n",
        "    width, height = image.size\n",
        "    ratio = min(224 / width, 224 / height)\n",
        "    image = image.resize((int(width * ratio), int(height * ratio)))\n",
        "    width, height = image.size\n",
        "    if height < 224:\n",
        "        # If width is shorter than height pad top and bottom.\n",
        "        top_padding = (224 - height) // 2\n",
        "        bottom_padding = 224 - height - top_padding\n",
        "        padded_image = Image.new('RGB', (width, 224), (255, 255, 255))\n",
        "        padded_image.paste(image, (0, top_padding))\n",
        "    else:\n",
        "        # Otherwise pad left and right.\n",
        "        left_padding = (224 - width) // 2\n",
        "        right_padding = 224 - width - left_padding\n",
        "        padded_image = Image.new('RGB', (224, height), (255, 255, 255))\n",
        "        padded_image.paste(image, (left_padding, 0))\n",
        "    return padded_image\n",
        "\n",
        "def scale_and_pad(original, pad_black=True):\n",
        "    ratio = min(224 / original.width, 224 / original.height)\n",
        "    original_np = np.array(original)\n",
        "    new_crop = original.resize((int(original.width * ratio), int(original.height * ratio)))\n",
        "    pixel_1 = original_np[1, 1]\n",
        "    pixel_2 = original_np[1, original_np.shape[-1]-1]\n",
        "    pixel_3 = original_np[original_np.shape[0]-1, 1]\n",
        "    pixel_4 = original_np[original_np.shape[0]-1, original_np.shape[-1]-1]\n",
        "    avg = np.rint(np.mean([pixel_1, pixel_2, pixel_3, pixel_4], axis=0)).astype(np.uint8)\n",
        "\n",
        "    color = tuple(avg) if not pad_black else (0, 0, 0)\n",
        "    new_image = Image.new(new_crop.mode, (224, 224), color)\n",
        "    dx = (224 - new_crop.width) // 2\n",
        "    dy = (224 - new_crop.height) // 2\n",
        "    new_image.paste(new_crop, (dx, dy))\n",
        "    return new_image, ratio, dx, dy, new_crop\n",
        "\n",
        "def encode_images_in_batches(images, batch_size=32):\n",
        "    def encode_image(image):\n",
        "        image_np = np.array(image)[:, :, :3]\n",
        "        encoded_jpeg = tf.io.encode_jpeg(image_np)\n",
        "        return tf.reshape(encoded_jpeg, (1,)), image_np\n",
        "\n",
        "    encoded_batches = []\n",
        "    original_batches = []\n",
        "\n",
        "    num_batches = len(images) // batch_size + (1 if len(images) % batch_size != 0 else 0)\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min(start_idx + batch_size, len(images))\n",
        "        current_batch = images[start_idx:end_idx]\n",
        "\n",
        "        encoded_batch = []\n",
        "        original_batch = []\n",
        "        for image in current_batch:\n",
        "            encoded, original = encode_image(image)\n",
        "            encoded_batch.append(encoded)\n",
        "            original_batch.append(original)\n",
        "\n",
        "        encoded_batches.append(tf.stack(encoded_batch))\n",
        "        original_batches.append(np.stack(original_batch))\n",
        "\n",
        "    return encoded_batches, original_batches\n",
        "\n",
        "def unpad_unscale_unrotate_uncrop(ink, ratio, dx, dy, min_x, min_y, angle):\n",
        "    transformed_strokes = []\n",
        "\n",
        "    for stroke in ink:\n",
        "        transformed_points = []\n",
        "        for point in stroke:\n",
        "            x_transformed = (point[0] - dx) / ratio\n",
        "            y_transformed = (point[1] - dy) / ratio\n",
        "\n",
        "            x_final = x_transformed + min_x\n",
        "            y_final = y_transformed + min_y\n",
        "\n",
        "            transformed_points.append((x_final, y_final))\n",
        "\n",
        "        transformed_strokes.append(Stroke(transformed_points))\n",
        "\n",
        "    transformed_ink = Ink(transformed_strokes)\n",
        "    return transformed_ink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "asB0-m_gVSMv"
      },
      "outputs": [],
      "source": [
        "# @title Model Preparation (hugging face model)\n",
        "\n",
        "# !wget https://storage.googleapis.com/derendering_model/small-p-cpu.zip\n",
        "# !unzip small-p-cpu.zip\n",
        "# model = tf.saved_model.load('small-p-cpu')\n",
        "\n",
        "from huggingface_hub import from_pretrained_keras\n",
        "\n",
        "model = from_pretrained_keras(\"Derendering/InkSight-Small-p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A36r4mf72U-0"
      },
      "outputs": [],
      "source": [
        "# @title Word level Inference\n",
        "# @markdown We use recognize and derender as default inference for demo and vanilla derender as fallback inference, check `Use_custom` to use your own **word-level** image\n",
        "from PIL import Image\n",
        "try:\n",
        "    from google.colab import files\n",
        "    in_colab = True\n",
        "except ImportError:\n",
        "    in_colab = False\n",
        "\n",
        "Use_custom = False # @param {type:\"boolean\"}\n",
        "\n",
        "if in_colab:\n",
        "    if Use_custom:\n",
        "        uploaded = files.upload()\n",
        "        input_image = Image.open(io.BytesIO(uploaded[list(uploaded.keys())[0]]))\n",
        "    else:\n",
        "        url = \"https://github.com/google-research/inksight/raw/main/test_inputs/word.jpg\"\n",
        "        response = requests.get(url)\n",
        "        input_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "else:\n",
        "    file_path = 'test_inputs/word.jpg'\n",
        "    input_image = Image.open(file_path)\n",
        "\n",
        "image, _, _, _, _ = scale_and_pad(input_image)\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P4o-iZVT2l7c"
      },
      "outputs": [],
      "source": [
        "# @title Word-level inference\n",
        "model = from_pretrained_keras(\"Derendering/InkSight-Small-p\")\n",
        "cf = model.signatures['serving_default']\n",
        "demo_prompt = \"Recognize and derender.\"\n",
        "fall_back_prompt = \"Derender the ink.\"\n",
        "\n",
        "input_text = tf.constant([demo_prompt], dtype=tf.string)\n",
        "image_encoded = tf.reshape(tf.io.encode_jpeg(np.array(image)[:, :, :3]), (1, 1))\n",
        "output = cf(**{'input_text': input_text, 'image/encoded': image_encoded})\n",
        "output_text = output[\"output_0\"].numpy()[0][0].decode()\n",
        "output_ink = detokenize(text_to_tokens(output_text))\n",
        "\n",
        "# Check if the ink is empty and try fallback prompt\n",
        "if len(output_ink.strokes) == 0:\n",
        "    print('Empty output, trying fallback prompt')\n",
        "    retry_input_text = tf.constant([fall_back_prompt], dtype=tf.string)\n",
        "    retry_output = cf(**{'input_text': retry_input_text, 'image/encoded': image_encoded})\n",
        "    retry_text = retry_output[\"output_0\"].numpy()[0][0].decode()\n",
        "    output_ink = detokenize(text_to_tokens(retry_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YQ6WS_Xw41s7"
      },
      "outputs": [],
      "source": [
        "# @title Result Visualization\n",
        "output_ink = detokenize(text_to_tokens(output['output_0'].numpy()[0][0].decode()))\n",
        "fig, ax = plt.subplots()\n",
        "plot_ink(output_ink, ax, input_image=load_and_pad_img(input_image))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EQXSrFKaVcU9"
      },
      "outputs": [],
      "source": [
        "# @title Full page pipeline with Tesseract or Doctr\n",
        "# @markdown To reproduce the results presented in the paper, we recommend using the Google Cloud Vision API. However, for free alternatives, we provide guidance on achieving similar outcomes using Tesseract or Doctr.\n",
        "# @markdown Check `Use_custom` to use your own **full-page** image.\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import io\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    in_colab = True\n",
        "except ImportError:\n",
        "    in_colab = False\n",
        "\n",
        "Use_custom = False  # @param {type:\"boolean\"}\n",
        "\n",
        "if in_colab:\n",
        "    if Use_custom:\n",
        "        uploaded = files.upload()\n",
        "        input_image = Image.open(io.BytesIO(uploaded[list(uploaded.keys())[0]]))\n",
        "        file_path = list(uploaded.keys())[0]\n",
        "    else:\n",
        "        url = \"https://github.com/google-research/inksight/raw/main/test_inputs/page.jpg\"\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        file_path = \"/content/page.jpg\"\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        input_image = Image.open(file_path)\n",
        "\n",
        "else:\n",
        "    file_path = 'test_inputs/page.jpg'\n",
        "    input_image = Image.open(file_path)\n",
        "\n",
        "input_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WqmqVK-eZiYF"
      },
      "outputs": [],
      "source": [
        "# @title Select word segmentor\n",
        "Segmentor = \"doctr\" # @param [\"tesseract\", \"doctr\"]\n",
        "word_imgs, word_info, bbox_img = extract_fullpage(file_path, option=Segmentor)\n",
        "bbox_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c4lfTNevZsc_"
      },
      "outputs": [],
      "source": [
        "# @title Full Page Batch Inference\n",
        "batchsize=32\n",
        "output_inks=[]\n",
        "model = from_pretrained_keras(\"Derendering/InkSight-Small-p\")\n",
        "cf = model.signatures['serving_default']\n",
        "demo_prompt = \"Recognize and derender.\"\n",
        "fall_back_prompt = \"Derender the ink.\"\n",
        "\n",
        "input_text = tf.constant([demo_prompt], dtype=tf.string)\n",
        "encode_word_imgs, original_word_imgs = encode_images_in_batches(word_imgs, batch_size=batchsize)\n",
        "output_batches = []\n",
        "t1 = time.perf_counter()\n",
        "for batch_img in tqdm(encode_word_imgs):\n",
        "    num_imgs_in_batch = batch_img.shape[0]\n",
        "\n",
        "    input_text = tf.constant([demo_prompt] * num_imgs_in_batch, dtype=tf.string)\n",
        "    output = cf(**{'input_text': input_text, 'image/encoded': batch_img})\n",
        "    output_batches.append(output)\n",
        "\n",
        "    # Process each image in the batch\n",
        "    for idx in range(output_batches[-1][\"output_0\"].shape[0]):\n",
        "        output_text = output_batches[-1][\"output_0\"].numpy()[idx][0].decode()\n",
        "        output_ink = detokenize(text_to_tokens(output_text))\n",
        "\n",
        "        # Check if the ink is empty\n",
        "        if len(output_ink.strokes) == 0:\n",
        "            retry_input_text = tf.constant([fall_back_prompt], dtype=tf.string)\n",
        "            retry_img = tf.expand_dims(batch_img[idx], 0)\n",
        "            retry_output = cf(**{'input_text': retry_input_text, 'image/encoded': retry_img})\n",
        "            retry_text = retry_output[\"output_0\"].numpy()[0][0].decode()\n",
        "            output_ink = detokenize(text_to_tokens(retry_text))\n",
        "\n",
        "\n",
        "        output_inks.append(output_ink)\n",
        "t2 = time.perf_counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCA99RQ3bAYP"
      },
      "outputs": [],
      "source": [
        "# @title Result Visualization\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def calculate_adaptive_line_width(word_info):\n",
        "    min_sides = []\n",
        "    for ratio, dx, dy, min_x, min_y, angle in word_info:\n",
        "        original_height = (224 - 2*dy) / ratio\n",
        "        original_width = (224 - 2*dx) / ratio\n",
        "\n",
        "        min_side = min(original_height, original_width)\n",
        "        min_sides.append(min_side)\n",
        "\n",
        "    mean_min_side = np.mean(min_sides)\n",
        "    lw = mean_min_side / 150\n",
        "\n",
        "    return lw\n",
        "\n",
        "max_side = max(input_image.size)\n",
        "lw = calculate_adaptive_line_width(word_info)\n",
        "# Set maximum dimensions for the figure\n",
        "MAX_FIG_WIDTH = 80\n",
        "MAX_FIG_HEIGHT = 80\n",
        "\n",
        "# Set path color for the ink\n",
        "path_color=\"white\"\n",
        "\n",
        "# Calculate scaled dimensions while maintaining aspect ratio\n",
        "img_width, img_height = input_image.size\n",
        "aspect_ratio = img_height / img_width\n",
        "\n",
        "if aspect_ratio < 0.5:\n",
        "    fig_height = 5\n",
        "    fig_width = min(fig_height / aspect_ratio, MAX_FIG_WIDTH)\n",
        "elif aspect_ratio > 2:\n",
        "    fig_width = min(MAX_FIG_WIDTH, 15)\n",
        "    fig_height = min(fig_width * aspect_ratio, MAX_FIG_HEIGHT)\n",
        "else:\n",
        "    fig_width = min(img_width / 100, MAX_FIG_WIDTH)  # Scale down by 100\n",
        "    fig_height = fig_width * aspect_ratio\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(3, 1, figsize=(fig_width, fig_height))\n",
        "\n",
        "\n",
        "\n",
        "def resize_image_if_needed(img, max_size=2000):\n",
        "    w, h = img.size\n",
        "    if max(w, h) > max_size:\n",
        "        scale = max_size / max(w, h)\n",
        "        new_size = (int(w * scale), int(h * scale))\n",
        "        return img.resize(new_size, Image.LANCZOS)\n",
        "    return img\n",
        "\n",
        "def scale_ink_coordinates(ink, original_size, new_size):\n",
        "    scale_x = new_size[0] / original_size[0]\n",
        "    scale_y = new_size[1] / original_size[1]\n",
        "\n",
        "    scaled_ink = copy.deepcopy(ink)\n",
        "    for stroke in scaled_ink.strokes:\n",
        "        stroke.x = [x * scale_x for x in stroke.x]\n",
        "        stroke.y = [y * scale_y for y in stroke.y]\n",
        "    return scaled_ink\n",
        "\n",
        "# Resize images for display\n",
        "original_size = input_image.size\n",
        "display_input = resize_image_if_needed(input_image)\n",
        "display_bbox = resize_image_if_needed(bbox_img)\n",
        "new_size = display_input.size\n",
        "\n",
        "all_inks = []\n",
        "for ink, (ratio, dx, dy, min_x, min_y, angle) in zip(output_inks, word_info):\n",
        "    recover_ink = unpad_unscale_unrotate_uncrop(ink, ratio, dx, dy, min_x, min_y, angle)\n",
        "    scaled_ink = scale_ink_coordinates(recover_ink, original_size, new_size)\n",
        "    all_inks.append(scaled_ink)\n",
        "\n",
        "ax[0].imshow(display_input)\n",
        "ax[0].set_title('Input Full page image')\n",
        "ax[1].imshow(display_bbox)\n",
        "ax[1].set_title(\"Bounding boxes\")\n",
        "ax[2].set_title(\"InkSight Result (Public Small-p)\")\n",
        "\n",
        "# Create darkened background\n",
        "enhancer = ImageEnhance.Brightness(display_input)\n",
        "dark_img = enhancer.enhance(0.45)\n",
        "ax[2].imshow(dark_img)\n",
        "\n",
        "# Optimize stroke drawing\n",
        "for ink in tqdm(all_inks, desc=\"Drawing inks\", total=len(all_inks)):\n",
        "    base_colors = plt.cm.get_cmap('rainbow', max(len(ink.strokes), 1))\n",
        "    for i, stroke in enumerate(ink.strokes):\n",
        "        x, y = np.array(stroke.x), np.array(stroke.y)\n",
        "\n",
        "        # Skip if too few points\n",
        "        if len(x) < 2:\n",
        "            continue\n",
        "\n",
        "        base_color = base_colors(len(ink.strokes) - 1 - i)\n",
        "        hsv_color = colorsys.rgb_to_hsv(*base_color[:3])\n",
        "        darker_color = colorsys.hsv_to_rgb(hsv_color[0], hsv_color[1], max(0, hsv_color[2] * 0.65))\n",
        "\n",
        "        # Simplify color array creation\n",
        "        alpha_values = np.linspace(0.5, 1.0, len(x))\n",
        "        colors = [mcolors.to_rgba(darker_color, alpha=a) for a in alpha_values]\n",
        "\n",
        "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "        lc = LineCollection(segments, colors=colors, linewidth=lw)\n",
        "        lc_two = LineCollection(segments, colors=colors, linewidth=lw)\n",
        "\n",
        "        # Simplified path effects\n",
        "        lc.set_path_effects([withStroke(linewidth=lw*1.25, foreground=path_color)])\n",
        "        lc_two.set_path_effects([withStroke(linewidth=lw*1.8)])\n",
        "\n",
        "        ax[2].add_collection(lc)\n",
        "        ax[1].add_collection(lc_two)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save with optimized settings\n",
        "fig.savefig('result.jpg', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(filename='result.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNfD7Md8KXJA"
      },
      "source": [
        "# Dataset Release: Generated Dataset Visualization\n",
        "\n",
        "In this section we show the generated digital inks of our model on 100 randomly selected test samples of publicly available IMGUR5K, IAM, HierText datasets, and also compare our models to \"golden\" human traced data.\n",
        "\n",
        "Structures of the Supplementary Materials for each public dataset (example below for IMGUR5k dataset, similar structure for HierText and IAM datasets):\n",
        "```\n",
        "‚îú‚îÄ‚îÄ IMGUR5k\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ images_sample\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 0rMi6_45.png\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 0wxvqTL_23.png\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ ...\n",
        "‚îú‚îÄ‚îÄ large-i_IMGUR5K_inkml\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ d+t\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 0rMi6_45.inkml\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 0wxvqTL_23.inkml\n",
        "‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ ...\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vanilla\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ r+d\n",
        "‚îú‚îÄ‚îÄ small-i_IMGUR5K_inkml\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ d+t\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vanilla\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ r+d\n",
        "‚îî‚îÄ‚îÄ small-p_IMGUR5K_inkml\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ d+t\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vanilla\n",
        "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ r+d\n",
        "```\n",
        "We store the raw input images in the folder with name that corresponds to each public dataset, and corresponding `.inkml` files in the folders with the naming convention `<model_name>_<dataset_name>_inkml`.\n",
        "\n",
        "\n",
        "Under each inkml folder there are three subfolders `d+t`, `vanilla`, and `r+d` corresponding to the data generated with the inference mode `Derender with Text`, `Vanilla Derendering`, and `Recognized and Derender`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SGBWMvVfOI9W"
      },
      "outputs": [],
      "source": [
        "# @title Notice\n",
        "from IPython.display import display, HTML\n",
        "display(HTML('<p style=\"font-size:20px; font-weight:bold; color:red; background-color:lightgray; padding:10px; width:50%\">Licence and Terms of Use</p>'))\n",
        "display(HTML('<p style=\"font-size:16px; width:50%\">Results of model inference on public datasets, the results of the human tracing, and the model itself are available under Apache V2 license for research, non-commerical usecases only, as the derivatives of non-commerical research datasets. </p>'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-uMp3ioRKWmp"
      },
      "outputs": [],
      "source": [
        "# @title Comparison between Inference Tasks\n",
        "from PIL import Image\n",
        "Dataset = \"HierText\" # @param [\"IMGUR5K\", \"IAM\", \"HierText\"]\n",
        "Num_samples = 3 # @param {type:\"integer\"}\n",
        "Model = \"Small-i\" # @param [\"Small-i\", \"Large-i\", \"Small-p\"]\n",
        "inkml_path = None\n",
        "if Model == \"Small-i\":\n",
        "    inkml_path = f\"./derendering_supp/small-i_{Dataset}_inkml\"\n",
        "elif Model == \"Small-p\":\n",
        "    inkml_path = f\"./derendering_supp/small-p_{Dataset}_inkml\"\n",
        "elif Model == \"Large-i\":\n",
        "    inkml_path = f\"./derendering_supp/large-i_{Dataset}_inkml\"\n",
        "else:\n",
        "    raise ValueError('Now only supports Small-i, Small-p, Large-i.')\n",
        "path = f\"./derendering_supp/{Dataset}/images_sample\"\n",
        "samples = os.listdir(path)\n",
        "picked_samples = random.sample(samples, Num_samples)\n",
        "\n",
        "plot_title = {\n",
        "    \"r+d\": \"Recognized: \",\n",
        "    \"d+t\": \"OCR Input: \",\n",
        "    \"vanilla\": \"Vanilla\"\n",
        "}\n",
        "query_modes = [\"d+t\", \"r+d\", \"vanilla\"]\n",
        "\n",
        "for name in picked_samples:\n",
        "    fig, ax = plt.subplots(1, 1+len(query_modes), figsize=(6*len(query_modes), 4))\n",
        "    img = load_and_pad_img_dir(os.path.join(path, name))\n",
        "    ax[0].set_xticks([])\n",
        "    ax[0].set_yticks([])\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title('Input')\n",
        "    for i, mode in enumerate(query_modes):\n",
        "        example_id = name.strip('.png')\n",
        "        inkml_file = os.path.join(inkml_path, mode, example_id + '.inkml')\n",
        "        ink = inkml_to_ink(inkml_file)\n",
        "        text_field = parse_inkml_annotations(inkml_file)['textField']\n",
        "\n",
        "        plot_ink(ink, ax[1+i], input_image=img, lw=1.8)\n",
        "        ax[1+i].set_xticks([])\n",
        "        ax[1+i].set_yticks([])\n",
        "        ax[1+i].set_title(f'{plot_title[mode]}{text_field}')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ctBXza8ULu4N"
      },
      "outputs": [],
      "source": [
        "# @title Comparison between Models using Derendering with Text\n",
        "from PIL import Image\n",
        "Dataset = \"IMGUR5K\" # @param [\"IMGUR5K\", \"IAM\", \"HierText\"]\n",
        "Num_samples = 3 # @param {type:\"integer\"}\n",
        "model_selections = [\"Small-p\", \"Small-i\", \"Large-i\"]\n",
        "\n",
        "path = f\"./derendering_supp/{Dataset}/images_sample\"\n",
        "samples = os.listdir(path)\n",
        "picked_samples = random.sample(list(samples), Num_samples)\n",
        "mode = 'd+t'\n",
        "\n",
        "for name in picked_samples:\n",
        "    fig, ax = plt.subplots(1, 1+len(query_modes), figsize=(4*(1+len(query_modes)), 4))\n",
        "    img = load_and_pad_img_dir(os.path.join(path, name))\n",
        "    ax[0].set_xticks([])\n",
        "    ax[0].set_yticks([])\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title('Input')\n",
        "    for i, model in enumerate(model_selections):\n",
        "        example_id = name.strip('.png')\n",
        "        inkml_path = f\"./derendering_supp/{model.lower()}_{Dataset}_inkml\"\n",
        "        inkml_file = os.path.join(inkml_path, mode, example_id + '.inkml')\n",
        "        ink = inkml_to_ink(inkml_file)\n",
        "        text_field = parse_inkml_annotations(inkml_file)['textField']\n",
        "\n",
        "        plot_ink(ink, ax[1+i], input_image=img, lw=1.8)\n",
        "        ax[1+i].set_xticks([])\n",
        "        ax[1+i].set_yticks([])\n",
        "        ax[1+i].set_title( model + \" | OCR Input: \" + text_field + ' ')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3-GG8J_lL0z-"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "human_ink = np.load('derendering_supp/human_tracing_hash_to_ink.npy', allow_pickle=True).item()\n",
        "all_samples = human_ink.keys()\n",
        "\n",
        "Dataset = \"HierText\"\n",
        "Num_samples = 3 # @param {type:\"integer\"}\n",
        "model_selections = [\"Small-p\", \"Small-i\", \"Large-i\"]\n",
        "\n",
        "path = f\"./derendering_supp/{Dataset}/images_sample\"\n",
        "samples = os.listdir(path)\n",
        "picked_samples = random.sample(list(all_samples), Num_samples)\n",
        "mode = 'd+t'\n",
        "\n",
        "for name in picked_samples:\n",
        "    fig, ax = plt.subplots(1, 1+len(query_modes)+1, figsize=(4*(1+len(query_modes)+1), 4))\n",
        "    img = load_and_pad_img_dir(os.path.join(path, name + '.png'))\n",
        "    ax[0].set_xticks([])\n",
        "    ax[0].set_yticks([])\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title('Input')\n",
        "    for i, model in enumerate(model_selections):\n",
        "        example_id = name.strip('.png')\n",
        "        inkml_path = f\"./derendering_supp/{model.lower()}_{Dataset}_inkml\"\n",
        "        inkml_file = os.path.join(inkml_path, mode, example_id + '.inkml')\n",
        "        ink = inkml_to_ink(inkml_file)\n",
        "        text_field = parse_inkml_annotations(inkml_file)['textField']\n",
        "\n",
        "        plot_ink(ink, ax[1+i], input_image=img, lw=1.8)\n",
        "        ax[1+i].set_xticks([])\n",
        "        ax[1+i].set_yticks([])\n",
        "        ax[1+i].set_title(\"OCR Input: \" + text_field + ' ' + model)\n",
        "        plot_ink(human_ink[example_id], ax[-1], input_image=img)\n",
        "        ax[-1].set_xticks([])\n",
        "        ax[-1].set_yticks([])\n",
        "        ax[-1].set_title('Human Traced')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b4ygJKUTL4BD"
      },
      "outputs": [],
      "source": [
        "human_ink = np.load('derendering_supp/human_tracing_hash_to_ink.npy', allow_pickle=True).item()\n",
        "all_samples = human_ink.keys()\n",
        "\n",
        "Dataset = \"HierText\"\n",
        "Num_samples = 3 # @param {type:\"integer\"}\n",
        "model_selections = [\"Small-p\", \"Small-i\", \"Large-i\"]\n",
        "\n",
        "path = f\"./derendering_supp/{Dataset}/images_sample\"\n",
        "samples = os.listdir(path)\n",
        "picked_samples = random.sample(list(all_samples), Num_samples)\n",
        "mode = 'd+t'\n",
        "\n",
        "for name in picked_samples:\n",
        "    fig, ax = plt.subplots(1, 1+len(query_modes)+1, figsize=(4*(1+len(query_modes)+1), 4))\n",
        "    img = load_and_pad_img_dir(os.path.join(path, name + '.png'))\n",
        "    ax[0].set_xticks([])\n",
        "    ax[0].set_yticks([])\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title('Input')\n",
        "    for i, model in enumerate(model_selections):\n",
        "        example_id = name.strip('.png')\n",
        "        inkml_path = f\"./derendering_supp/{model.lower()}_{Dataset}_inkml\"\n",
        "        inkml_file = os.path.join(inkml_path, mode, example_id + '.inkml')\n",
        "        ink = inkml_to_ink(inkml_file)\n",
        "        text_field = parse_inkml_annotations(inkml_file)['textField']\n",
        "\n",
        "        plot_ink(ink, ax[1+i], input_image=img, lw=1.8)\n",
        "        ax[1+i].set_xticks([])\n",
        "        ax[1+i].set_yticks([])\n",
        "        ax[1+i].set_title(\"OCR Input: \" + text_field + ' ' + model)\n",
        "        plot_ink(human_ink[example_id], ax[-1], input_image=img)\n",
        "        ax[-1].set_xticks([])\n",
        "        ax[-1].set_yticks([])\n",
        "        ax[-1].set_title('Human Traced')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}